{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#nice-to-meet-you","title":"Nice to Meet You","text":""},{"location":"#my-name-is-dahye-kang","title":"My name is Dahye Kang","text":""},{"location":"#junior-software-engineer","title":"Junior Software Engineer","text":"<p> @kangdaia |  kangdaia |  Blog</p> <p> kangdiamond@gmail.com</p> <p></p> <p>Creative problem-solver who seeks a career in Software Engineering position to hone skills in object-oriented programming using Python, C++, and Java, and building data structures, algorithms, and software design with the full-stack experience.</p> <p>I am a collaborative person who enjoys sharing thoughts and new ideas, and I am always open to asking and answering questions with colleagues. I am a self-starter, motivated by the new challenges I face at work, and always eager to learn new technologies.</p>"},{"location":"#tech-spec","title":"Tech Spec","text":"Programming Language Python, JavaScript, SQL, Swift, Java, Ruby, C++, R, SAS, Ocaml, C, C#, Objective-C Frameworks React, Django, Express, SwiftUI, Vue, Node, PyTorch Platforms &amp; Tools GitHub, Google Cloud Platform (GCP), Firebase, AWS, Linux, XCode, VScode, Docker Design Tools Adobe XD, Photoshop, Illustrator, Premier Pro"},{"location":"#experience","title":"Experience","text":""},{"location":"#technology-engineer-intern","title":"Technology Engineer Intern","text":""},{"location":"#ibm-client-engineering-team","title":"IBM Client Engineering Team","text":""},{"location":"#seoul-korea-202307-present","title":"Seoul, Korea |  2023.07 ~ Present","text":"<ul> <li>Working on Fast-API backend, Studying Openshift, Kafka!</li> </ul>"},{"location":"#data-engineering-devcourse-bootcamp","title":"Data Engineering DevCourse (Bootcamp)","text":""},{"location":"#programmerscokr-grepp","title":"Programmers.co.kr (Grepp)","text":""},{"location":"#seoul-korea-202304-202308","title":"Seoul, Korea |  2023.04 ~ 2023.08","text":"<ul> <li>Studying to build data infrastructure and job-related skills such as Spark, Airflow, AWS cloud services, and Docker, which are skills required for a data engineer career.</li> <li>Worked on two team projects based on learning: WORDSIGHT, Animal STAT-US Tracker</li> </ul>"},{"location":"#education","title":"Education","text":""},{"location":"#university-of-maryland-bachelor-of-science","title":"University of Maryland, Bachelor of Science","text":""},{"location":"#computer-science-major-minor-statistics","title":"Computer Science Major, minor Statistics","text":""},{"location":"#college-park-md-2018-2021","title":"College Park, MD |  2018 ~ 2021","text":""},{"location":"#green-river-college-associate-of-science","title":"Green River College, Associate of Science","text":""},{"location":"#computer-science-major","title":"Computer Science Major","text":""},{"location":"#auburn-wa-2015-2018","title":"Auburn, WA |  2015 ~ 2018","text":""},{"location":"projects/project-animal/","title":"Animal STAT-US Tracker","text":"<p> Website |   Github</p> <p>Data Engineering DevCourse  Data Warehouse &amp; Dashboard Construct Project</p> <p>Team Member: @kangdaia @ih-tjpark @Jeon-peng @usiohc</p>"},{"location":"projects/project-animal/#overview","title":"Overview","text":"<p>Developed a service that can see abandoned animal infographics and current protecting animal posts using the abandoned animal and animal shelter data from Korean government public API.</p>"},{"location":"projects/project-animal/#tech-stack","title":"Tech Stack","text":"Field Stack Design FrameWork Hosting &amp; GA Data Management Google Cloud Storage, Bigquery, FireStore Dashboard Superset Data Pipelines CI/CD Testing Cloud Tools"},{"location":"projects/project-animal/#result","title":"Result","text":""},{"location":"projects/project-happy/","title":"What Makes You Happy?","text":"<p>CMSC320: Intro to Data Science  Final Tutorial Project</p> <p>Fall 2020, Dahye Kang</p>"},{"location":"projects/project-happy/#introduction","title":"Introduction","text":"<p>Happiness is one of the most important parts of human life. We feel happiness from pleasure or joy instantaneously and feel happiness from life satisfaction. Long-term life satisfaction is one of the most important factors in human life and influence how society is satisfied, not only the individual. Since 2012, based on the survey, people digitize the answer to measure happiness from life satisfaction.</p> <p>Every year, the World Happiness Report releases global happiness data for each country. The reports review the state of happiness in the world today and show how the digitized data of happiness explains personal and national variations in happiness. Due to the efficiency and reliability of happiness measurements, the report is used in organizations and civil society to inform their policy-making decisions.</p> <p>The scores are based on answers to the main life evaluation question asked in the poll from Gallup World Poll. The questions ask for answers in a range of 0 to 10 that zero is the worst possible life, and ten is the best possible life. In the data, the columns following the happiness score estimate the extent to which each of six factors -- economy (GDP per Capita), social support (family), healthy life expectancy, freedom, absence of corruption, and generosity; these explain why some countries rank higher than others.</p> <p>In this instruction, we will focus on how each factor influences a high or low score, which leads to a high or low rank of happiness, and see the priority factor of happiness; Money, healthy, or freedom?</p>"},{"location":"projects/project-happy/#0-getting-started","title":"0. Getting Started","text":"<p>All the work is based on Python 3 (Python 3.9.0) with the following packages:</p> <ul> <li>pandas:<ul> <li>a module that use to make a date frame, organize, and do other     various work for data table; necessary for data handling.</li> </ul> </li> <li>numpy:<ul> <li>a module to use for multi-dimension array. Required when dealing     with linear algebra calculation.</li> </ul> </li> <li>matplotlib:<ul> <li>a module to visualize the data; can make various graphs. We only     use the pyplot from matplotlib package.</li> </ul> </li> <li>seaborn:<ul> <li>data visualization library based on matplotlib, but more various     and high-level plot can use to visualize the data</li> </ul> </li> <li>sklearn:<ul> <li>a module for machine learning to use to learn and predict the     model in various way; linear regression, classfication, etc.</li> </ul> </li> </ul> <p>If you are new to this, install the modules using <code>pip install [module_name]</code> (this is one way to install a module; there is various way to install).</p> <p>Import the modules like this in the code:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_validate\nimport seaborn as sns\n</code></pre> <p>After calling all the necessary modules, we need to download the dataset. Each year, the World Happiness Report release the dataset, but as we cannot download all the file individually, we use kaggle.com to download already collected data for 2015 to 2019 by other users.</p> <p>Download dataset in this link: World_Happiness</p>"},{"location":"projects/project-happy/#1-data-processing","title":"1. Data Processing","text":""},{"location":"projects/project-happy/#11-reading-data-from-file","title":"1.1 Reading Data from File","text":"<p>In this section, the goal is reading data from CSV files, reorganizing the data as we want. First, as each year data is in separate csv file, we need to read each csv file. Read the CSV files for each year by pd.read_csv(filename) method, and DO NOT add all data up yet; each file has differet columns and columns\\' name. Also, the original CSV files do not have a column indicating year, I add the year column to the DataFrame using insert(location, column_name, column_value, allow_duplicate = True).</p> <pre><code>years = [2015, 2016, 2017, 2018, 2019]\n# read 2015.csv\ndata_2015 = pd.read_csv(f\"WorldHappiness/2015.csv\", sep=\",\")\ndata_2015.insert(0, \"year\", 2015, True)\n# read 2016.csv\ndata_2016 = pd.read_csv(f\"WorldHappiness/2016.csv\", sep=\",\")\ndata_2016.insert(0, \"year\", 2016, True)\n# read 2017.csv\ndata_2017 = pd.read_csv(f\"WorldHappiness/2017.csv\", sep=\",\")\ndata_2017.insert(0, \"year\", 2017, True)\n# read 2018.csv\ndata_2018 = pd.read_csv(f\"WorldHappiness/2018.csv\", sep=\",\")\ndata_2018.insert(0, \"year\", 2018, True)\n# read 2019.csv\ndata_2019 = pd.read_csv(f\"WorldHappiness/2019.csv\", sep=\",\")\ndata_2019.insert(0, \"year\", 2019, True)\n</code></pre> <p>Before making data efficient, we need to check that each dataset\\'s columns are different or equivalent. Here is the shape of each year dataset:</p> <ul> <li>2015 Data Shape: (158, 13)</li> <li>2016 Data Shape: (157, 14)</li> <li>2017 Data Shape: (155, 13)</li> <li>2018 Data Shape: (156, 10)</li> <li>2019 Data Shape: (156, 10)</li> </ul> <p>Each year data has some different columns, so to make all year data to one, we only use the columns that exist in all data set. Below is the table showing which year\\'s data is missing or named differently:</p> 2015 2016 2017 2018 2019 Country Country Country Country or region Country or region Region Region - - - Happiness Rank Happiness Rank Happiness.Rank Happiness Rank Happiness Rank Happiness Score Happiness Score Happiness Score Happiness Score Happiness Score Standard Error - - - - - Lower Confidence Interval Whisker.low - - - Upper Confidence Interval Whisker.low - - Economy (GDP per Capita) Economy (GDP per Capita) Economy..GDP.per.Capita. GDP per capita GDP per capita Family Family Family Social support Social support Health (Life Expectancy) Health (Life Expectancy) Health..life.Expectancy. Healthy lift expenctancy Healthy life expectancy Freedom Freedom Freedom Freedom to make life chices Freedom to make life chices Trust (Government Corruption) Trust (Government Corruption) Trust..Government.Corruption. Perceptions of Corruption Perceptions of Corruption Generosity Generosity Generosity Generosity Generosity Dystopia Residual Dystopia Residual Dystopia.Residual - - <p>In summary, we will use the columns: country, region, rank, score, gdp, family, lifeExp, freedom, trust, and generosity. There is no region data in 2017, 2018, and 2019, but the region data is useful to analyze the data by grouping it, we will merge the 2015 and 2016 region data to 2017, 2018, and 2019.</p>"},{"location":"projects/project-happy/#12-fill-the-empty-column","title":"1.2 Fill the Empty Column","text":"<p>To get the region data from 2015 and 2016, sort the two datasets by Country name, and extract only the Country and Region columns. Then, the 2015 data and 2016 data have different length, so we need to consider the case that the countries only exist in one of the dataset. To prevent loss, we will merge the two region data as outer option in pd.merge function.</p> <pre><code>#Sort data by Country\nsort_2016 = data_2016.sort_values(by=\"Country\")\nsort_2015 = data_2015.sort_values(by=\"Country\")\n#Get the Country and Region columns only\nregion_2016 = sort_2016[[\"Country\", \"Region\"]]\nregion_2015 = sort_2015[[\"Country\", \"Region\"]]\n#Merge two\nregion = pd.merge(region_2015,region_2016, how=\"outer\")\n</code></pre> <ul> <li>region and country shape: (164, 2)</li> </ul> <p>Now, the region has more region data than data from 2015 and 2016. After making the region dataset, we will apply this region data to different year datasets. The 2018 and 2019 dataset have different column name for Country as Country or region (check the above column list table), specifing the merge option that the column in left dataset to merge is \\\"Country or region\\\" and the column in right dataset to merge is \\\"Country\\\". The merge between the 2017, 2018, and 2019 data and region data is inner merge (default); it will not include the row which cannot find corresponding country and region in region data.</p> <pre><code>data_new_2017 = pd.merge(data_2017, region)\ndata_new_2018 = pd.merge(data_2018, region, left_on=\"Country or region\", right_on=\"Country\")\ndata_new_2019 = pd.merge(data_2019, region, left_on=\"Country or region\", right_on=\"Country\")\n</code></pre> <p>The outcome dataset shows the shape as:</p> <ul> <li>2015 New Data Shape: (158, 13)</li> <li>2016 New Data Shape: (157, 14)</li> <li>2017 New Data Shape: (153, 14)</li> <li>2018 New Data Shape: (154, 12)</li> <li>2019 New Data Shape: (152, 12)</li> </ul>"},{"location":"projects/project-happy/#13-extract-and-rename","title":"1.3 Extract and Rename","text":"<p>As above I said, we only need the data that are all common in every year data.</p> <pre><code>#columns list to want to use\ncols = [\"year\",\"country\",\"region\",\"rank\",\"score\",\"gdp\",\"family\",\"lifeExp\",\"freedom\",\"trust\",\"generosity\"]\n#loop each year dataset and append the necessary data to list\ndata = []\nfor i, row in data_2015.iterrows():\napp = row[[\"year\",\"Country\",\"Region\",\"Happiness Rank\",\"Happiness Score\",\"Economy (GDP per Capita)\",\n\"Family\",\"Health (Life Expectancy)\",\"Freedom\",\"Trust (Government Corruption)\",\"Generosity\"]]\ndata.append(app.array)\nfor i, row in data_2016.iterrows():\ndata.append(row[[\"year\",\"Country\",\"Region\",\"Happiness Rank\",\"Happiness Score\",\"Economy (GDP per Capita)\",\n\"Family\",\"Health (Life Expectancy)\",\"Freedom\",\"Trust (Government Corruption)\",\n\"Generosity\"]].array)\nfor i, row in data_new_2017.iterrows():\ndata.append(row[[\"year\",\"Country\",\"Region\",\"Happiness.Rank\",\"Happiness.Score\",\n\"Economy..GDP.per.Capita.\",\"Family\", \"Health..Life.Expectancy.\",\"Freedom\",\n\"Trust..Government.Corruption.\",\"Generosity\"]].array)\nfor i, row in data_new_2018.iterrows():\ndata.append(row[[\"year\",\"Country or region\",\"Region\",\"Overall rank\",\"Score\",\n\"GDP per capita\", \"Social support\", \"Healthy life expectancy\", \n\"Freedom to make life choices\", \"Perceptions of corruption\", \"Generosity\"]].array)\nfor i, row in data_new_2019.iterrows():\ndata.append(row[[\"year\",\"Country or region\",\"Region\",\"Overall rank\",\"Score\",\"GDP per capita\", \n\"Social support\", \"Healthy life expectancy\", \"Freedom to make life choices\",\n\"Perceptions of corruption\", \"Generosity\"]].array)\ndata= pd.DataFrame(data,columns=cols)\ndata.head()\n</code></pre> year country region rank score gdp family lifeExp freedom trust generosity 0 2015 Switzerland Western Europe 1 7.587 1.39651 1.34951 0.94143 0.66557 0.41978 1 2015 Iceland Western Europe 2 7.561 1.30232 1.40223 0.94784 0.62877 0.14145 2 2015 Denmark Western Europe 3 7.527 1.32548 1.36058 0.87464 0.64938 0.48357 3 2015 Norway Western Europe 4 7.522 1.45900 1.33095 0.88521 0.66973 0.36503 4 2015 Canada North America 5 7.427 1.32629 1.32261 0.90563 0.63297 0.32957 <p>Now, let's see how the data is related.</p>"},{"location":"projects/project-happy/#2-data-exploration-and-analysis","title":"2. Data Exploration and Analysis","text":"<p>The goal of this section is analyzing data as finding the relation of six factors and happiness score. To see the global trend of relation with factors and score, we plot the heatmap.</p>"},{"location":"projects/project-happy/#21-global-trend","title":"2.1 Global Trend","text":"<p>To the dataset what we want to see the correlation, use the corr() and return a correlation matrix. Using the seaborn, plot the heatmap given the values as the correlation matrix.</p> <pre><code>#Corrleation Matrix\ncorrMatrix = data[[\"score\",\"gdp\",\"family\",\"lifeExp\",\"freedom\",\"trust\",\"generosity\"]].corr()\n#plot heatmap\nplt.figure(figsize = (10, 7))\nsns.heatmap(corrMatrix, annot=True, square=True)\nplt.title(\"Correlation of Happiness Score with Other factors\", fontweight = \"bold\", fontsize = 16)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.show()\n</code></pre> <p></p> <p>The heatmap shows the correlations with the level of strongness through the color difference. The color bar on right side indicate which color is closed to 1 or 0. According to the heatmap, the score shows the correlation coefficient as 0.79 for gdp, 0.65 for family, 0.74 for lifeExp, 0.55 for freedom, 0.4 for trust, 0.14 for generoisty. The order of strong coefficient is GDP &gt; Family &gt; Life Expectency &gt; Freedom &gt; perceptions of corruption &gt; Generoisty.</p> <p>In the Statistics, above 0.8 correlation coefficient is strong, above 0.5 and below 0.8 correlation coefficient is consider moderate, and below 0.5 correlation coefficient is weak. Then, the gdp, family, lifeExp, and freedom is considered moderate coefficient with no strong coefficient. Leave the weak correlation coefficient, we will see that how the four variable is related to score with region indication.</p>"},{"location":"projects/project-happy/#22-happiness-score-vs-gdp","title":"2.2 Happiness Score vs. GDP","text":"<p>We will plot the multi linear regression line with scatter plot using the lmplot in seaborn. The first plot we will look at is happiness score vs. GDP. Set x value as gdp and y value is score with hue as region (show regions in different colors). Set col is year and col_wrap to 3 which are showing graph for each year data with three columns. Then we can plot the 5 subplots of scatter plot with linear regression line for each region.</p> <pre><code>plt.figure(figsize=(12, 10))\ng = sns.lmplot(data = data, y = \"score\", x = \"gdp\", hue = \"region\", col = \"year\", col_wrap = 3, height = 6)\ng.fig.suptitle(\"Happiness Score vs. GDP per Time\", fontsize = 35, fontweight = \"bold\")\ng.set_titles(col_template=\"{col_name}\", size = 20)\ng.set_yticklabels(fontsize = 10)\ng.set_axis_labels(\"GDP per Capita\", \"Happiness Score\", size = 18)\ng.fig.subplots_adjust(wspace = 0.3, hspace = 0.4, top = 0.90, left = 0.1)\ng.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>The plot shows that the tendency of higher GDP with higher happiness score in every year. If we see the each region\\'s regression line, some regions do not follow the global tendency. The North America and Australia and New Zealand show that their score does not get high by high GDP. The Western Europ shows the steep slope among other regions which means high gdp countries have high score of happiness. Compared to it, the Sub-Saharan Africa and Southern Asia show the GDP does not influence much to the high score.</p>"},{"location":"projects/project-happy/#23-happniess-score-vs-life-expectency","title":"2.3 Happniess Score vs. Life Expectency","text":"<p>Do the same way with the modification of x variable to lifeExp</p> <pre><code>plt.figure(figsize=(12,10))\ng = sns.lmplot(data=data, y=\"score\", x=\"lifeExp\", hue=\"region\", palette=\"Set1\", col = \"year\", col_wrap = 3, height=6)\ng.fig.suptitle(\"Happiness Score vs. Life Expectancy per Time\", fontsize = 35, fontweight = \"bold\")\ng.set_titles(col_template=\"{col_name}\", size = 20)\ng.set_yticklabels(fontsize = 10)\ng.set_axis_labels(\"Health (Life Expectancy)\", \"Happiness Score\", size = 18)\ng.fig.subplots_adjust(wspace = 0.3, hspace = 0.4, top = 0.90, left = 0.1)\ng.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>The overall trend is that with higher lifeExp, the higher happiness score is which is smaller than GDP slope. The Sub-Saharan Africa, Central and Eastern Europe, and Western Europe show that same or similar score with various lifeExp value meaning lifeExp does not influence much on happiness. However, Middle East and Northern Africa and Latin America and Caribbean show that high lifeExp reflect to score as high.</p>"},{"location":"projects/project-happy/#24-happiness-score-vs-social-support","title":"2.4 Happiness Score vs. Social Support","text":"<p>Do same as above plots with the modification of x variable.</p> <pre><code>plt.figure(figsize=(12,10))\ng = sns.lmplot(data = data, y =\"score\", x = \"family\", hue = \"region\", palette=\"CMRmap\", col = \"year\", col_wrap = 3, height=6)\ng.fig.suptitle(\"Happiness Score vs. Social Support per Time\", fontsize = 35, fontweight = \"bold\")\ng.set_titles(col_template=\"{col_name}\", size = 20)\ng.set_yticklabels(fontsize = 10)\ng.set_axis_labels(\"Social Support\", \"Happiness Score\", size = 18)\ng.fig.subplots_adjust(wspace = 0.3, hspace = 0.4, top = 0.90, left = 0.1)\ng.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>This is also shows the high social support with high score but not much as the lifeExp graph. Most of country does not reflect the high happiness score with high social support, but the Western Europe reflect it.</p>"},{"location":"projects/project-happy/#25-happiness-score-vs-freedom","title":"2.5 Happiness Score vs. Freedom","text":"<p>Do same as above plots with modification of x variable</p> <pre><code>plt.figure(figsize=(12,10))\ng = sns.lmplot(data=data, y=\"score\", x=\"freedom\", hue=\"region\", palette=\"twilight\", col = \"year\", col_wrap = 3, height=6)\ng.fig.suptitle(\"Happiness Score vs. Freedom per Time\", fontsize = 35, fontweight = \"bold\")\ng.set_titles(col_template=\"{col_name}\", size = 20)\ng.set_yticklabels(fontsize = 10)\ng.set_axis_labels(\"Freedom\", \"Happiness Score\", size = 18)\ng.fig.subplots_adjust(wspace = 0.3, hspace = 0.4, top = 0.90, left = 0.1)\ng.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>This shows the weak relations between happiness score and freedom that most of country shows similar happiness score whatever the freedom gets.</p>"},{"location":"projects/project-happy/#26-heatmap-per-region","title":"2.6 Heatmap per Region","text":"<p>The above plots show that the factors influenced to the happiness score are different for each region. To see the correlation of factors and score for each region, we will plot the 10 subplots of heatmaps.</p> <pre><code>#region list - 10 regions\nregion = data[\"region\"].unique()\n#subplots of 2x5 matrixs\nfig, ax = plt.subplots(2, 5, figsize = (25, 10), constrained_layout = True, sharey = True, sharex = True)\nj = 0\n#show only one color bar in given location\ncbar_ax = fig.add_axes([.9, .3, .02, .4])\n#loop over the region list\nfor i, loc in enumerate(region):\nif i&gt;0 and i%5 == 0:\nj+=1\n#correlation matrix for corresponding region\ncorrMatrix = data[data[\"region\"]==loc][[\"score\",\"gdp\",\"family\",\"lifeExp\",\"freedom\",\"trust\",\"generosity\"]].corr()\n#heatmap for correlation matrix\nsns.heatmap(corrMatrix, cbar=i == 0, ax= ax[j][i%5],vmin=0,annot=True, cbar_ax=None if i else cbar_ax)\nax[j][i%5].set_title(loc, loc = \"center\", fontsize = 15, fontweight = 10)\nfig.tight_layout(rect=[0, 0, .98, 1])\nfig.suptitle(\"Correlation of Happiness with Other Factors for Each Region\", fontsize = 30, fontweight = \"bold\")\nplt.show()\n</code></pre> <p></p> <p>The chart listed the top three factors which is high correlation for each region (indicate strong coefficient as <code>&lt;span style=\"color:#EF9342\"&gt;</code>color<code>&lt;/span&gt;</code>, moderate coefficient as <code>&lt;span style=\"color:#C70039\"&gt;</code>color<code>&lt;/span&gt;</code>):</p> Region First Second Third Global GDP lifeExp Social Support Western Europe trust freedom GDP North America trust freedom generosity Austraila and New Zealand trust generosity freedom Middle East and Northern Africa GDP freedom lifeExp Latin America and Caribbean GDP freedom lifeExp Southeastern Asia GDP lifeExp social support Central and Eastern Europe family freedom GDP Eastern Asia GDP lifeExp trust Sub-Saharan Africa family GDP freedom Southern Asia trust family lifeExp <p>Unlike the Global's strongest coefficient is GDP, the regions follow it is only 4 out of 10. Assume that the country has enough economy growth (high GDP), consider the perceptions of corruption and freedom to the important factors for happiness; perceiving social values are important. Also, for the country considered the GDP as first, they are also care about the healthy life expectancy. So, I can assume that importance of ecomonic growth comes with factor of healty life expectancy for the happiness.</p>"},{"location":"projects/project-happy/#3-model-prediction","title":"3. Model Prediction","text":"<p>Based on above interpretation, We will find the model for global and plot the residual scatter plot. Also, we will find the model per each region, and see how well they predict compared to using global model. All the finding is only based on top three facters for every model.</p>"},{"location":"projects/project-happy/#31-global-model","title":"3.1 Global Model","text":"<p>We will use the LinearRegression method under sklearn.linear_model for multi linear regression. For the global model, the x variable is gdp, lifeExp, and family and y variable is score. Apply x and y to linEq.fit(x,y) and get the slope and intercept.</p> <pre><code>linEq = linear_model.LinearRegression()\nx = data[[\"gdp\",\"lifeExp\",\"family\"]]\ny = data[\"score\"]\nlinEq.fit(x,y)\nslope = linEq.coef_ #slope coefficient list\nintercept = linEq.intercept_\nprint(f\"Equation: score = {intercept:.4f} + {slope[0]:.4f}*(gdp) + {slope[1]:.4f}*(lifeExp) + {slope[2]:.4f}*(family)\")\n</code></pre> <pre><code>Equation: score = 2.6423 + 1.2529*(gdp) + 1.1586*(lifeExp) + 0.8188*(family)\n</code></pre> <p>Now we get the expected y value based on original x using linEq.predict(x), and calculate residual by subracting original y to expected y value. Then, add region column and original y column; it will be used for plotting per each region.</p> <pre><code>predict_y = linEq.predict(x)\nresidual = np.subtract(y, predict_y)\nresidual = pd.DataFrame(np.array(residual), columns=[\"difference\"])\nresidual[\"region\"] = data[\"region\"]\nresidual[\"original\"] = y\nresidual[\"expect\"] = predict_y\nresidual.head()\n</code></pre> difference region original expect 0 0.999329 Western Europe 7.587 1 1.040749 Western Europe 7.561 2 1.096641 Western Europe 7.527 3 0.936365 Western Europe 7.522 4 0.990811 North America 7.427 <p>Let's see how global model works in each region.</p> <p>We will plot 10 subplots of residual plots using scatterplot.</p> <pre><code>#region list\nregion = data[\"region\"].unique()\nfig, ax = plt.subplots(2, 5, figsize = (25, 10), constrained_layout = True, sharey = True, sharex = True)\nj = 0\nfor i, loc in enumerate(region):\nif i&gt;0 and i%5 == 0: #update row\nj+=1\n#scatter plot\nsns.scatterplot(x=\"expect\", y=\"difference\", size=5, palette=\"ch:r=-.2,d=.3_r\", ax= ax[j][i%5], data=residual[residual[\"region\"]==loc])\nax[j][i%5].set_title(loc, loc = \"center\", fontsize = 18, fontweight = 10)\nplt.show()\n</code></pre> <p></p> <p>Only the Southeastern Asia show acceptable residual plot, and rest of plot show the pattern which mean the model is not working well.</p> <p>To see the accurate score of model prediction, use the cross_validate(regression_method, x, y), and get the test score of it with default 5-fold CV.</p> <p><pre><code>result = cross_validate(linEq, x, y)\nglobal_square = np.array(result[\"test_score\"])\nprint(\"R-Squared Score for global model in Avg: \",global_square.mean())\n</code></pre> R-Squared Score for global model in Avg:  0.4221315680210174</p>"},{"location":"projects/project-happy/#32-predict-per-region","title":"3.2 Predict per Region","text":"<p>Now we will find the model for each region based on top three factors for each. Use a dict to save the top three factor for each region, and find the model like above.</p> <pre><code># make a table for top three factors each\ntop_three = {\"region\":region,\n\"first\":\n[\"trust\",\"trust\",\"trust\",\"gdp\",\"gdp\",\n\"gdp\",\"family\",\"gdp\",\"family\",\"trust\"],\n\"second\":\n[\"freedom\",\"freedom\",\"generosity\",\"freedom\",\n\"freedom\",\"lifeExp\",\"freedom\",\"lifeExp\",\"gdp\",\n\"family\"],\n\"third\":\n[\"gdp\",\"generosity\",\"freedom\",\"lifeExp\",\"lifeExp\",\n\"family\",\"gdp\",\"trust\",\"freedom\",\"lifeExp\"]}\ntop_three = pd.DataFrame(top_three)\ntop_three.head()\n</code></pre> region first second third 0 Western Europe trust freedom 1 North America trust freedom 2 Australia and New Zealand trust generosity 3 Middle East and Northern Africa gdp freedom 4 Latin America and Caribbean gdp freedom <p>For each region, it get the top three factors list, do the multi linear regression, predict y value based on original x, get the score of prediction, and plot a scatter plot for residual vs. predicted value.</p> <pre><code>fig, ax = plt.subplots(2, 5, figsize = (25, 10), constrained_layout = True, sharey = True, sharex = True)\nj = 0\nscore = []\nfor i, loc in enumerate(region):\nif i&gt;0 and i%5 == 0: #update row\nj+=1\n#get three factors for certain region\nfactors = top_three[top_three[\"region\"]==loc][[\"first\",\"second\",\"third\"]]\n#multi linear regression\nx = data[data[\"region\"]==loc][factors.to_numpy().flatten()]\ny_loc = data[data[\"region\"]==loc][\"score\"]\nlinEq.fit(x, y_loc)\n#predict\npredict_y = linEq.predict(x)\nresidual = np.subtract(y_loc, predict_y)\nresidual = pd.DataFrame(np.array(residual), columns=[\"difference\"])\nresidual[\"original\"] = y_loc.reset_index(drop=True)\nresidual[\"expect\"] = predict_y\n#get the score of prediction\nscore.append(linEq.score(x,y_loc))\n#scatter plot for residual vs. predicted value\nsns.scatterplot(x=\"expect\", y=\"difference\", size=5, color=\"green\", ax= ax[j][i%5], data=residual)\nax[j][i%5].set_title(loc, loc = \"center\", fontsize = 18, fontweight = 10)\nplt.show()\n</code></pre> <p></p> <p>This graph shows much scattered points in each plot, which means the model is working better than just using global model. Still, the Central and Eastern Europe and Sub-Saharan Africa tend to clustered together vertically, it is not the best fit model to them.</p> <p>Unlike the global model, we do not use cross-validate due to small number of data for each region. Below is the R-Square score for each region:</p> <pre><code>for i, loc in enumerate(region):\nprint(f\"R-Squared Score for {loc:31} {score[i]:.4f}\")\n</code></pre> <pre><code>R-Squared Score for Western Europe                  0.7637\nR-Squared Score for North America                   0.9650\nR-Squared Score for Australia and New Zealand       0.4905\nR-Squared Score for Middle East and Northern Africa 0.7752\nR-Squared Score for Latin America and Caribbean     0.6679\nR-Squared Score for Southeastern Asia               0.7681\nR-Squared Score for Central and Eastern Europe      0.4753\nR-Squared Score for Eastern Asia                    0.7523\nR-Squared Score for Sub-Saharan Africa              0.2967\nR-Squared Score for Southern Asia                   0.4608s\n</code></pre> <p>It shows that the model fit well for North America, and the Wstern Europe, Middle East and Northern Africa, Southeastern Asia, and Eastern Asia show the model is acceptable. It does not work well for Sub-saharan Africa.</p>"},{"location":"projects/project-happy/#conclusion","title":"Conclusion","text":"<p>The main goal of instruction is finding out which factors is most reflected in the happiness score. We used the top three factors to see how the model fit, and get various score of prediction for each region. Then, let\\'s think about why some region get high score and others not.</p> <p>In the data analysis step, through the heatmaps per region, we check that which factors are strong correlation coefficient or not. The North America shows first and second factors are strong coefficient, and thrid factor is moderate. This means all three factors highly related to happiness score. So, the prediction using these factors shows 0.965 R-Squared score. Contrast to it, the Sub-Saharan Africa\\'s top three factors are all weak correlation coefficient. Leading to it, the score is also the lowest, meaning that the factors does not related to happiness score much. These brings the idea that the higher correlation coefficient is, the higher score get from the prediction based on the variable.</p> <p>If we consider this condition, the Western Europe get the highest influence to the happiness score from perception of corruption, the North America get the highest influence from perception of corruption and freedom, the Middle East and Northern Africa get the highest influence from economy (GDP per Capita), and also the Southeastern Asia get the highest influence from economy (GDP per Capita). The rest of region has the coefficient of moderate or weak, so it will not reflected well in the happiness score.</p> <p>Through the insctruction, we explore the data processing, data analyzing, and model prediction based on data. Hope this is useful information, and Thank you for sharing time to read.</p> <p>Thank you.</p>"},{"location":"projects/project-tsp/","title":"Analysis of Solving Traveling Salesman Problem (TSP)","text":"<p> Document</p> <p>CMSC 421: Intro to Artificial Intelligence  TSP Solving Analysis Report</p> <p>Fall 2020, Dahye Kang</p>"},{"location":"projects/project-tsp/#overview","title":"Overview","text":"<p>Implemented A* with various heuristic solving the Traveling Salesman Problem (TSP); Uniform Cost Search, Random Edges, Cheapest Remaining Edges, Minimum Spanning Trees, Hill Climbing, Simulated Annealing, and Generic Algorithm, and analyzed the test results based on total costs and number of nodes expanded for each algorithm.</p>"},{"location":"projects/project-tsp/#introduction","title":"Introduction","text":"<p>Using the Python in Jupyter notebook, I implemented 1) A as Uniform Cost Search, random edges, and cheapest remaining edges, 2) A with Minimum-spanning-tree (MST) heuristic, and 3) local search algorithms which are hill-climbing, simulated annealing, and genetic algorithm methods. After implementation, tested the accuracy of each algorithm by three different size of 30 TSP graphs (size 5, size 10, and size 15); cannot proceed with larger size due to system limitation.</p>"},{"location":"projects/project-tsp/#part-1","title":"PART. 1","text":"<p>Based on implementation of A* with Uniform Cost Search, Random edges, and Cheapest remaining edges, analyzed the test results by total cost to take and number of nodes expanded for each algorithm with size 5 and 10.</p> <p>Figure 1 compares the total cost (minimum, average, maximum) and node expanded by the Uniform cost search (UCS), Random Edges (Random), and Cheapest remaining edges (CRE) algorithms with 2 different sizes (5 and 10).</p> <p>// FIG1</p> <p>In the size=5, the highest average cost is from random remaining heuristic, and the highest number of nodes expanded is from UCS. In the size=10, also the random heuristic's average cost is the highest, and the UCS's average number of nodes expanded is the highest. It is not emphasized in size 5, but in size 10, the random heuristic's difference between min and max is the largest; meaning the output of total cost is varied in a wide range. Also, for the UCS's number of nodes expanded, it shows a larger difference in Min and Max for each size.</p> <p>The figure 2 shows the comparison between the real and CPU runtime for each algorithm for each size. Which showing the smallest runtime on average is random edges for both size 5 and 10. UCS takes less time than cheap edges, but there is no big difference in min and max runtime. Depend on the size of the graph, the runtime shows a huge difference. Both UCS and cheapest show 10 times more than the size 5 average runtime in size 10. The random one shows around 5 times more than the size 5 average runtime in size 10. The interesting thing is that my cheap heuristic's real runtime has a very high max value compared to other data. There are some factors to consider: 1) my heuristic is not the optimal algorithm and 2) may have more than square or longer runtime in some helper functions. The other data shows that CPU runtime and real runtime have a very small difference on average. However, when comparing the max runtime, there is some difference between CPU and real runtime. This is stronger in the larger sizes.</p> <p>// FIG 2</p>"},{"location":"projects/project-tsp/#part-2","title":"PART. 2","text":"<p>Analyzed the test result of Minimum-spanning-tree (MST) implementation by the ratio of total cost from MST to total cost from CRE and ratio of node expanded from MST to CRE with size 5, 10, and 15. Before proceeding, the data is collected under a fixed starting node to make a better cost comparison on the same starting point.</p> <p>// FIGH 3, 4</p> <p>Both figure 3 and 4 show that as the larger size, the smaller ratio of total cost and number of nodes expanded. If the ratio of the total cost is closed to zero, then the MST total cost is smaller compared to the CRE total cost. In the size 5, there is no big difference between the total cost of MST and CRE (some total cost for MST is larger than CRE), but in the size 15, the ratio is 0.9173 meaning the total cost of MST is around 90 percent of the total cost of CRE. This is more standing out in the graph for the ratio of nodes expanded. If the ratio of the number of nodes expanded is closed to zero, then the MST node expanded is smaller than CRE. The ratio in size 5 is also small as that around 50% of nodes expanded in CRE is the number of nodes expanded in MST. In the size 15, the ratio is 0.2246 meaning the number of nodes expanded in MST is around 20% of the number of nodes expanded in CRE. Two different results show that the MST heuristic dominates the CRE heuristic.</p> <p>Moreover, analyzed the test result of Minimum-spanning-tree (MST) implementation by the difference of total cost from MST and algorithms from Part 1 to best solution\u2019s costs among algorithms in part 1. Number of nodes expanded is compared with same method in size 5, 10, and 15. The figure 5 is the difference in total cost between UCS, Random, and CRE, with MST. It shows that as the size the larger, the difference becomes larger. The difference is the largest between Random heuristic and MST in every size group.</p> <p>The figure 6 shows the difference in node expanded between UCS, Random, and CRE, with MST. In size 5, it is hard to find the difference between the A heuristics and MST, but in the size 15, the average node expanded is growing up. However, the Min value shows that both total cost and node expanded if MST is not always a better result. Sometimes A* makes the smaller total cost of node expanded, but in the frequency that how many times Figure 3. Ratio of total cost from MST to CRE for size 5, 10, and 15</p> <p>Figure 4. Ratio of number of nodes expanded from MST to CRE for size 5, 10, and 15 make a better result, the MST is much better.</p> <p>// FIGH 5, 6</p>"},{"location":"projects/project-tsp/#part-3","title":"PART. 3","text":"<p>//FIGH 7</p> <p>Implemented the three local search algorithms; hill-climbing (HC), simulated annealing (SA), and generic algorithm (GA), and analyzed test result by comparing difference of total cost with these and MST and CPU runtime for each algorithm.</p> <p>//FIGH 8</p> <p>The points in the HC are very spread out overall, but many points in SA are one a similar runtime range. Both HC and SA have a similar difference in total cost (not much good compared to MST), but SA shows faster runtime than HC; meaning the quickest solution under the same condition solution is the SA.</p> <p>//FIG 9</p> <p>Also, the GA is quite fast, but there is a problem with accuracy. I do the process that returns 0 costs if the path is not acceptable, and among 30 graphs, 18 graphs show 0 costs. So only 40% is an acceptable result for GA. If considering these, the fastest and the most accurate algorithm based on my code is the SA.</p>"},{"location":"projects/project-wordsight/","title":"WORDSIGHT","text":"<p> Website (Slow) |   Github</p> <p>Data Engineering DevCourse  Web Crawling Project</p> <p>Team Member: @kangdaia @hyeonji32 @ih-tjpark @joyunji</p>"},{"location":"projects/project-wordsight/#overview","title":"Overview","text":"<p>Developed news trends searching service by keyword through data crawling and natural language processing.</p> <p>News Data from: BigKinds</p>"},{"location":"projects/project-wordsight/#tech-stack","title":"Tech Stack","text":"Field Stack Crawling NLP Frontend Backend Tool"},{"location":"projects/project-wordsight/#result","title":"Result","text":""}]}